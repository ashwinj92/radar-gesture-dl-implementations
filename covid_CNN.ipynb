{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0um2uudKdBEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166d1d8e-1e29-4d77-d9fe-a27d85dd2eab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7TeOWteuYG"
      },
      "source": [
        "path_ml = \"/content/drive/My Drive/Covid/Rock/ML\"\n",
        "path_pa = \"/content/drive/My Drive/Covid/Rock/PA\"\n",
        "\n",
        "# Used to set the number of decimal places for common features\n",
        "decimal_round = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez30Hx5VezHW"
      },
      "source": [
        "# Read feature set from Rock Spectroscopy\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "fname = []\n",
        "file = os.path.join(path_ml,'Andesite73302_200AVG.txt')\n",
        "with open(file) as firstFile:\n",
        "  rock_features = []\n",
        "  for line in firstFile:\n",
        "    columns = line.split() # split line into parts\n",
        "    if len(columns) > 1:   # if at least 2 parts/columns\n",
        "       rock_features.append(round(float(columns[0]), decimal_round))\n",
        "df = pd.DataFrame(columns=rock_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_HV8rfgEFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "facad4b0-f0ad-4516-c1aa-0c7f22ea3b88"
      },
      "source": [
        "# Read data from rock spectroscopes\n",
        "from PIL import Image\n",
        "import csv\n",
        "lst = []\n",
        "dfp = pd.DataFrame() \n",
        "for dirpath, dirs, files in os.walk(path_ml):\n",
        "    for filename in files:\n",
        "        fname = os.path.join(dirpath,filename)\n",
        "        data = []      \n",
        "        with open(fname) as inf:         \n",
        "          dfp = dfp.append(pd.Series(np.loadtxt(inf)[:, 1]), ignore_index=True)\n",
        "\n",
        "for dirpath, dirs, files in os.walk(path_pa):\n",
        "    for filename in files:\n",
        "        fname = os.path.join(dirpath,filename)\n",
        "        data = []      \n",
        "        with open(fname) as inf:         \n",
        "          dfp = dfp.append(pd.Series(np.loadtxt(inf)[:, 1]), ignore_index=True)            \n",
        "\n",
        "dfp.columns = rock_features\n",
        "dfp.shape\n",
        "dfp.head(70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>198.066</th>\n",
              "      <th>198.122</th>\n",
              "      <th>198.177</th>\n",
              "      <th>198.233</th>\n",
              "      <th>198.288</th>\n",
              "      <th>198.344</th>\n",
              "      <th>198.399</th>\n",
              "      <th>198.455</th>\n",
              "      <th>198.511</th>\n",
              "      <th>198.566</th>\n",
              "      <th>198.622</th>\n",
              "      <th>198.677</th>\n",
              "      <th>198.733</th>\n",
              "      <th>198.788</th>\n",
              "      <th>198.844</th>\n",
              "      <th>198.899</th>\n",
              "      <th>198.955</th>\n",
              "      <th>199.010</th>\n",
              "      <th>199.066</th>\n",
              "      <th>199.121</th>\n",
              "      <th>199.177</th>\n",
              "      <th>199.232</th>\n",
              "      <th>199.288</th>\n",
              "      <th>199.343</th>\n",
              "      <th>199.399</th>\n",
              "      <th>199.454</th>\n",
              "      <th>199.510</th>\n",
              "      <th>199.565</th>\n",
              "      <th>199.621</th>\n",
              "      <th>199.676</th>\n",
              "      <th>199.732</th>\n",
              "      <th>199.787</th>\n",
              "      <th>199.842</th>\n",
              "      <th>199.898</th>\n",
              "      <th>199.953</th>\n",
              "      <th>200.009</th>\n",
              "      <th>200.064</th>\n",
              "      <th>200.120</th>\n",
              "      <th>200.175</th>\n",
              "      <th>200.230</th>\n",
              "      <th>...</th>\n",
              "      <th>967.438</th>\n",
              "      <th>967.507</th>\n",
              "      <th>967.577</th>\n",
              "      <th>967.647</th>\n",
              "      <th>967.716</th>\n",
              "      <th>967.786</th>\n",
              "      <th>967.855</th>\n",
              "      <th>967.925</th>\n",
              "      <th>967.994</th>\n",
              "      <th>968.064</th>\n",
              "      <th>968.133</th>\n",
              "      <th>968.203</th>\n",
              "      <th>968.272</th>\n",
              "      <th>968.342</th>\n",
              "      <th>968.411</th>\n",
              "      <th>968.480</th>\n",
              "      <th>968.550</th>\n",
              "      <th>968.619</th>\n",
              "      <th>968.689</th>\n",
              "      <th>968.758</th>\n",
              "      <th>968.827</th>\n",
              "      <th>968.897</th>\n",
              "      <th>968.966</th>\n",
              "      <th>969.035</th>\n",
              "      <th>969.105</th>\n",
              "      <th>969.174</th>\n",
              "      <th>969.243</th>\n",
              "      <th>969.312</th>\n",
              "      <th>969.382</th>\n",
              "      <th>969.451</th>\n",
              "      <th>969.520</th>\n",
              "      <th>969.589</th>\n",
              "      <th>969.658</th>\n",
              "      <th>969.727</th>\n",
              "      <th>969.797</th>\n",
              "      <th>969.866</th>\n",
              "      <th>969.935</th>\n",
              "      <th>970.004</th>\n",
              "      <th>970.073</th>\n",
              "      <th>970.142</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-8.337</td>\n",
              "      <td>-7.891</td>\n",
              "      <td>-7.746</td>\n",
              "      <td>-7.965</td>\n",
              "      <td>-8.041</td>\n",
              "      <td>-7.819</td>\n",
              "      <td>-7.649</td>\n",
              "      <td>-8.002</td>\n",
              "      <td>-8.117</td>\n",
              "      <td>-7.886</td>\n",
              "      <td>-7.705</td>\n",
              "      <td>-7.808</td>\n",
              "      <td>-7.671</td>\n",
              "      <td>-7.766</td>\n",
              "      <td>-7.868</td>\n",
              "      <td>-7.861</td>\n",
              "      <td>-7.657</td>\n",
              "      <td>-7.553</td>\n",
              "      <td>-7.632</td>\n",
              "      <td>-7.733</td>\n",
              "      <td>-7.768</td>\n",
              "      <td>-7.793</td>\n",
              "      <td>-7.842</td>\n",
              "      <td>-7.599</td>\n",
              "      <td>-7.455</td>\n",
              "      <td>-7.302</td>\n",
              "      <td>-7.313</td>\n",
              "      <td>-7.087</td>\n",
              "      <td>-6.977</td>\n",
              "      <td>-7.154</td>\n",
              "      <td>-7.493</td>\n",
              "      <td>-7.274</td>\n",
              "      <td>-7.234</td>\n",
              "      <td>-7.120</td>\n",
              "      <td>-7.170</td>\n",
              "      <td>-7.274</td>\n",
              "      <td>-7.144</td>\n",
              "      <td>-7.289</td>\n",
              "      <td>-7.153</td>\n",
              "      <td>-7.098</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.858</td>\n",
              "      <td>-8.212</td>\n",
              "      <td>-8.094</td>\n",
              "      <td>-8.046</td>\n",
              "      <td>-8.277</td>\n",
              "      <td>-8.156</td>\n",
              "      <td>-8.375</td>\n",
              "      <td>-8.442</td>\n",
              "      <td>-8.336</td>\n",
              "      <td>-8.516</td>\n",
              "      <td>-8.544</td>\n",
              "      <td>-8.375</td>\n",
              "      <td>-8.067</td>\n",
              "      <td>-8.460</td>\n",
              "      <td>-8.317</td>\n",
              "      <td>-8.168</td>\n",
              "      <td>-8.023</td>\n",
              "      <td>-8.512</td>\n",
              "      <td>-8.419</td>\n",
              "      <td>-8.265</td>\n",
              "      <td>-8.381</td>\n",
              "      <td>-8.632</td>\n",
              "      <td>-8.539</td>\n",
              "      <td>-8.519</td>\n",
              "      <td>-8.355</td>\n",
              "      <td>-8.583</td>\n",
              "      <td>-8.359</td>\n",
              "      <td>-8.474</td>\n",
              "      <td>-8.407</td>\n",
              "      <td>-8.449</td>\n",
              "      <td>-8.648</td>\n",
              "      <td>-8.764</td>\n",
              "      <td>-8.562</td>\n",
              "      <td>-8.603</td>\n",
              "      <td>-8.548</td>\n",
              "      <td>-8.485</td>\n",
              "      <td>-8.657</td>\n",
              "      <td>-8.627</td>\n",
              "      <td>-8.568</td>\n",
              "      <td>-8.466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.161</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.742</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.532</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.997</td>\n",
              "      <td>1.167</td>\n",
              "      <td>1.119</td>\n",
              "      <td>1.031</td>\n",
              "      <td>1.145</td>\n",
              "      <td>1.083</td>\n",
              "      <td>1.066</td>\n",
              "      <td>1.194</td>\n",
              "      <td>1.130</td>\n",
              "      <td>1.195</td>\n",
              "      <td>1.092</td>\n",
              "      <td>0.970</td>\n",
              "      <td>1.012</td>\n",
              "      <td>1.343</td>\n",
              "      <td>1.397</td>\n",
              "      <td>1.365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.866</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.843</td>\n",
              "      <td>0.948</td>\n",
              "      <td>0.988</td>\n",
              "      <td>1.003</td>\n",
              "      <td>1.005</td>\n",
              "      <td>1.011</td>\n",
              "      <td>1.023</td>\n",
              "      <td>0.846</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.841</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.653</td>\n",
              "      <td>0.721</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.622</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.104</td>\n",
              "      <td>-1.103</td>\n",
              "      <td>-1.265</td>\n",
              "      <td>-1.117</td>\n",
              "      <td>-1.027</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>-1.041</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>-0.945</td>\n",
              "      <td>-0.762</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.173</td>\n",
              "      <td>-0.886</td>\n",
              "      <td>-0.988</td>\n",
              "      <td>-0.901</td>\n",
              "      <td>-1.119</td>\n",
              "      <td>-1.129</td>\n",
              "      <td>-1.032</td>\n",
              "      <td>-0.992</td>\n",
              "      <td>-0.908</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-0.343</td>\n",
              "      <td>-0.366</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>-0.451</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>-0.642</td>\n",
              "      <td>-0.697</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.398</td>\n",
              "      <td>-0.398</td>\n",
              "      <td>0.032</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>-0.453</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-0.507</td>\n",
              "      <td>-0.451</td>\n",
              "      <td>-0.634</td>\n",
              "      <td>-0.566</td>\n",
              "      <td>-0.442</td>\n",
              "      <td>-0.615</td>\n",
              "      <td>-0.718</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.636</td>\n",
              "      <td>-0.570</td>\n",
              "      <td>-0.773</td>\n",
              "      <td>-0.958</td>\n",
              "      <td>-0.780</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>-0.839</td>\n",
              "      <td>-0.878</td>\n",
              "      <td>-0.865</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>-0.781</td>\n",
              "      <td>-1.009</td>\n",
              "      <td>-0.983</td>\n",
              "      <td>-0.619</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>-0.843</td>\n",
              "      <td>-1.012</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>-0.754</td>\n",
              "      <td>-0.921</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>-0.679</td>\n",
              "      <td>-0.905</td>\n",
              "      <td>-1.046</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-1.018</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>-1.095</td>\n",
              "      <td>-0.954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>-0.855</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>-0.699</td>\n",
              "      <td>-0.801</td>\n",
              "      <td>-0.829</td>\n",
              "      <td>-0.895</td>\n",
              "      <td>-1.082</td>\n",
              "      <td>-1.083</td>\n",
              "      <td>-0.598</td>\n",
              "      <td>-0.646</td>\n",
              "      <td>-1.063</td>\n",
              "      <td>-1.076</td>\n",
              "      <td>-1.124</td>\n",
              "      <td>-0.824</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>-0.957</td>\n",
              "      <td>-0.803</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.762</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-0.608</td>\n",
              "      <td>-0.423</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.305</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>-0.301</td>\n",
              "      <td>-0.553</td>\n",
              "      <td>-0.331</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>-0.372</td>\n",
              "      <td>-0.333</td>\n",
              "      <td>-0.483</td>\n",
              "      <td>-0.118</td>\n",
              "      <td>-0.131</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>...</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.054</td>\n",
              "      <td>-0.049</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>-0.147</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>-0.233</td>\n",
              "      <td>-0.258</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>-0.559</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>-0.451</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>-0.678</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.552</td>\n",
              "      <td>-0.528</td>\n",
              "      <td>-0.552</td>\n",
              "      <td>-0.728</td>\n",
              "      <td>-0.549</td>\n",
              "      <td>-0.421</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>-0.399</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-0.636</td>\n",
              "      <td>-0.577</td>\n",
              "      <td>-0.588</td>\n",
              "      <td>-0.687</td>\n",
              "      <td>-0.436</td>\n",
              "      <td>-0.695</td>\n",
              "      <td>-0.494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.419</td>\n",
              "      <td>-0.310</td>\n",
              "      <td>-0.170</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.406</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.036</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.359</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.788</td>\n",
              "      <td>1.567</td>\n",
              "      <td>1.742</td>\n",
              "      <td>1.999</td>\n",
              "      <td>1.891</td>\n",
              "      <td>1.745</td>\n",
              "      <td>1.523</td>\n",
              "      <td>1.661</td>\n",
              "      <td>1.819</td>\n",
              "      <td>1.960</td>\n",
              "      <td>1.625</td>\n",
              "      <td>1.717</td>\n",
              "      <td>1.860</td>\n",
              "      <td>1.782</td>\n",
              "      <td>1.723</td>\n",
              "      <td>1.682</td>\n",
              "      <td>1.710</td>\n",
              "      <td>...</td>\n",
              "      <td>0.920</td>\n",
              "      <td>0.831</td>\n",
              "      <td>0.649</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.904</td>\n",
              "      <td>1.013</td>\n",
              "      <td>0.998</td>\n",
              "      <td>1.013</td>\n",
              "      <td>0.948</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.766</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.686</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.264</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.331</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.390</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.362</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>-0.084</td>\n",
              "      <td>-0.095</td>\n",
              "      <td>-0.255</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.155</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>0.091</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.147</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.410</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.012</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.763</td>\n",
              "      <td>1.507</td>\n",
              "      <td>1.747</td>\n",
              "      <td>1.604</td>\n",
              "      <td>1.521</td>\n",
              "      <td>1.640</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.601</td>\n",
              "      <td>1.624</td>\n",
              "      <td>1.535</td>\n",
              "      <td>1.740</td>\n",
              "      <td>1.697</td>\n",
              "      <td>1.825</td>\n",
              "      <td>1.807</td>\n",
              "      <td>1.708</td>\n",
              "      <td>1.612</td>\n",
              "      <td>1.555</td>\n",
              "      <td>...</td>\n",
              "      <td>1.545</td>\n",
              "      <td>1.451</td>\n",
              "      <td>1.324</td>\n",
              "      <td>1.369</td>\n",
              "      <td>1.184</td>\n",
              "      <td>1.148</td>\n",
              "      <td>1.478</td>\n",
              "      <td>1.358</td>\n",
              "      <td>1.553</td>\n",
              "      <td>1.428</td>\n",
              "      <td>1.145</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.936</td>\n",
              "      <td>0.967</td>\n",
              "      <td>1.168</td>\n",
              "      <td>0.911</td>\n",
              "      <td>0.921</td>\n",
              "      <td>0.809</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.627</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.467</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-1.139</td>\n",
              "      <td>-1.133</td>\n",
              "      <td>-1.095</td>\n",
              "      <td>-1.047</td>\n",
              "      <td>-0.777</td>\n",
              "      <td>-0.889</td>\n",
              "      <td>-1.076</td>\n",
              "      <td>-0.979</td>\n",
              "      <td>-0.990</td>\n",
              "      <td>-0.732</td>\n",
              "      <td>-0.968</td>\n",
              "      <td>-0.743</td>\n",
              "      <td>-0.706</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.696</td>\n",
              "      <td>-0.944</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>-0.592</td>\n",
              "      <td>-0.752</td>\n",
              "      <td>-0.778</td>\n",
              "      <td>-0.683</td>\n",
              "      <td>-0.742</td>\n",
              "      <td>-0.870</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.312</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.384</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.489</td>\n",
              "      <td>0.497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.013</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>0.016</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>-0.242</td>\n",
              "      <td>-0.230</td>\n",
              "      <td>-0.278</td>\n",
              "      <td>-0.341</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>-0.350</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.358</td>\n",
              "      <td>-0.295</td>\n",
              "      <td>-0.267</td>\n",
              "      <td>-0.219</td>\n",
              "      <td>-0.323</td>\n",
              "      <td>-0.375</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>-0.628</td>\n",
              "      <td>-0.434</td>\n",
              "      <td>-0.444</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>-0.623</td>\n",
              "      <td>-0.672</td>\n",
              "      <td>-0.738</td>\n",
              "      <td>-0.659</td>\n",
              "      <td>-0.716</td>\n",
              "      <td>-0.598</td>\n",
              "      <td>-0.684</td>\n",
              "      <td>-0.750</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.628</td>\n",
              "      <td>-0.582</td>\n",
              "      <td>-0.541</td>\n",
              "      <td>-0.935</td>\n",
              "      <td>-0.769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-0.114</td>\n",
              "      <td>0.015</td>\n",
              "      <td>-0.235</td>\n",
              "      <td>-0.148</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.167</td>\n",
              "      <td>-0.182</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.137</td>\n",
              "      <td>-0.163</td>\n",
              "      <td>-0.020</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.197</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>-0.034</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.661</td>\n",
              "      <td>0.770</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.454</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.520</td>\n",
              "      <td>0.622</td>\n",
              "      <td>0.390</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.702</td>\n",
              "      <td>0.805</td>\n",
              "      <td>...</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.419</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.074</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.142</td>\n",
              "      <td>-0.048</td>\n",
              "      <td>-0.130</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.025</td>\n",
              "      <td>-0.042</td>\n",
              "      <td>-0.055</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.056</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>-0.191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-0.964</td>\n",
              "      <td>-1.003</td>\n",
              "      <td>-0.840</td>\n",
              "      <td>-0.857</td>\n",
              "      <td>-0.732</td>\n",
              "      <td>-0.834</td>\n",
              "      <td>-1.051</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.570</td>\n",
              "      <td>-0.882</td>\n",
              "      <td>-0.693</td>\n",
              "      <td>-0.848</td>\n",
              "      <td>-0.816</td>\n",
              "      <td>-0.853</td>\n",
              "      <td>-0.626</td>\n",
              "      <td>-0.639</td>\n",
              "      <td>-0.629</td>\n",
              "      <td>-0.737</td>\n",
              "      <td>-0.907</td>\n",
              "      <td>-0.713</td>\n",
              "      <td>-0.898</td>\n",
              "      <td>-0.722</td>\n",
              "      <td>-0.880</td>\n",
              "      <td>-0.263</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.129</td>\n",
              "      <td>-0.167</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.132</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.063</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>-0.239</td>\n",
              "      <td>-0.156</td>\n",
              "      <td>-0.127</td>\n",
              "      <td>0.015</td>\n",
              "      <td>-0.153</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.185</td>\n",
              "      <td>-0.113</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>-0.155</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>-0.529</td>\n",
              "      <td>-0.558</td>\n",
              "      <td>-0.340</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.466</td>\n",
              "      <td>-0.449</td>\n",
              "      <td>-0.408</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>-0.449</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>-0.598</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>-0.678</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>-0.536</td>\n",
              "      <td>-0.378</td>\n",
              "      <td>-0.639</td>\n",
              "      <td>-0.790</td>\n",
              "      <td>-0.541</td>\n",
              "      <td>-0.537</td>\n",
              "      <td>-0.548</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-8.201</td>\n",
              "      <td>-8.058</td>\n",
              "      <td>-7.873</td>\n",
              "      <td>-8.060</td>\n",
              "      <td>-8.006</td>\n",
              "      <td>-7.828</td>\n",
              "      <td>-7.916</td>\n",
              "      <td>-7.943</td>\n",
              "      <td>-7.929</td>\n",
              "      <td>-7.902</td>\n",
              "      <td>-7.841</td>\n",
              "      <td>-7.903</td>\n",
              "      <td>-7.780</td>\n",
              "      <td>-7.912</td>\n",
              "      <td>-7.925</td>\n",
              "      <td>-7.877</td>\n",
              "      <td>-7.825</td>\n",
              "      <td>-7.806</td>\n",
              "      <td>-7.780</td>\n",
              "      <td>-7.924</td>\n",
              "      <td>-7.864</td>\n",
              "      <td>-7.836</td>\n",
              "      <td>-7.909</td>\n",
              "      <td>-7.648</td>\n",
              "      <td>-7.370</td>\n",
              "      <td>-7.295</td>\n",
              "      <td>-7.372</td>\n",
              "      <td>-7.373</td>\n",
              "      <td>-7.351</td>\n",
              "      <td>-7.354</td>\n",
              "      <td>-7.293</td>\n",
              "      <td>-7.206</td>\n",
              "      <td>-7.212</td>\n",
              "      <td>-7.226</td>\n",
              "      <td>-7.292</td>\n",
              "      <td>-7.390</td>\n",
              "      <td>-7.291</td>\n",
              "      <td>-7.369</td>\n",
              "      <td>-7.112</td>\n",
              "      <td>-7.049</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.967</td>\n",
              "      <td>-8.108</td>\n",
              "      <td>-7.987</td>\n",
              "      <td>-8.030</td>\n",
              "      <td>-8.225</td>\n",
              "      <td>-8.164</td>\n",
              "      <td>-8.104</td>\n",
              "      <td>-8.265</td>\n",
              "      <td>-8.185</td>\n",
              "      <td>-8.275</td>\n",
              "      <td>-8.239</td>\n",
              "      <td>-8.126</td>\n",
              "      <td>-8.098</td>\n",
              "      <td>-8.202</td>\n",
              "      <td>-8.100</td>\n",
              "      <td>-8.239</td>\n",
              "      <td>-8.283</td>\n",
              "      <td>-8.329</td>\n",
              "      <td>-8.323</td>\n",
              "      <td>-8.297</td>\n",
              "      <td>-8.307</td>\n",
              "      <td>-8.289</td>\n",
              "      <td>-8.376</td>\n",
              "      <td>-8.302</td>\n",
              "      <td>-8.317</td>\n",
              "      <td>-8.414</td>\n",
              "      <td>-8.421</td>\n",
              "      <td>-8.409</td>\n",
              "      <td>-8.513</td>\n",
              "      <td>-8.450</td>\n",
              "      <td>-8.494</td>\n",
              "      <td>-8.556</td>\n",
              "      <td>-8.456</td>\n",
              "      <td>-8.505</td>\n",
              "      <td>-8.480</td>\n",
              "      <td>-8.548</td>\n",
              "      <td>-8.604</td>\n",
              "      <td>-8.476</td>\n",
              "      <td>-8.455</td>\n",
              "      <td>-8.519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 13490 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    198.066  198.122  198.177  198.233  ...  969.935  970.004  970.073  970.142\n",
              "0    -8.337   -7.891   -7.746   -7.965  ...   -8.657   -8.627   -8.568   -8.466\n",
              "1     0.161    0.435    0.240    0.122  ...    0.334    0.501    0.286    0.189\n",
              "2    -1.104   -1.103   -1.265   -1.117  ...   -0.867   -0.931   -1.095   -0.954\n",
              "3    -0.994   -0.923   -0.855   -0.707  ...   -0.687   -0.436   -0.695   -0.494\n",
              "4    -0.419   -0.310   -0.170    0.032  ...    0.204    0.346    0.206    0.124\n",
              "..      ...      ...      ...      ...  ...      ...      ...      ...      ...\n",
              "65   -0.084   -0.095   -0.255   -0.193  ...    0.074    0.121    0.191    0.199\n",
              "66   -1.139   -1.133   -1.095   -1.047  ...   -0.582   -0.541   -0.935   -0.769\n",
              "67   -0.114    0.015   -0.235   -0.148  ...   -0.036    0.056   -0.039   -0.191\n",
              "68   -0.964   -1.003   -0.840   -0.857  ...   -0.632   -0.896   -0.730   -0.714\n",
              "69   -8.201   -8.058   -7.873   -8.060  ...   -8.604   -8.476   -8.455   -8.519\n",
              "\n",
              "[70 rows x 13490 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVu-R2J4ZyJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e7fa741f-6ff7-47d1-d759-1aee64d19616"
      },
      "source": [
        "#Read feature set for covid data of patients\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "columns = defaultdict(list)\n",
        "\n",
        "path_nnt = \"/content/drive/My Drive/Covid/patients/covid/NNT\"\n",
        "path_nt = \"/content/drive/My Drive/Covid/patients/covid/NT\"\n",
        "path_p = \"/content/drive/My Drive/Covid/patients/covid/P\"\n",
        "\n",
        "fname = []\n",
        "file = os.path.join(path_nnt,'17_Sel_Mean.csv')\n",
        "df = pd.DataFrame()\n",
        "cols = pd.read_csv(file, header=None, usecols=[0]).T\n",
        "cols.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>24512</th>\n",
              "      <th>24513</th>\n",
              "      <th>24514</th>\n",
              "      <th>24515</th>\n",
              "      <th>24516</th>\n",
              "      <th>24517</th>\n",
              "      <th>24518</th>\n",
              "      <th>24519</th>\n",
              "      <th>24520</th>\n",
              "      <th>24521</th>\n",
              "      <th>24522</th>\n",
              "      <th>24523</th>\n",
              "      <th>24524</th>\n",
              "      <th>24525</th>\n",
              "      <th>24526</th>\n",
              "      <th>24527</th>\n",
              "      <th>24528</th>\n",
              "      <th>24529</th>\n",
              "      <th>24530</th>\n",
              "      <th>24531</th>\n",
              "      <th>24532</th>\n",
              "      <th>24533</th>\n",
              "      <th>24534</th>\n",
              "      <th>24535</th>\n",
              "      <th>24536</th>\n",
              "      <th>24537</th>\n",
              "      <th>24538</th>\n",
              "      <th>24539</th>\n",
              "      <th>24540</th>\n",
              "      <th>24541</th>\n",
              "      <th>24542</th>\n",
              "      <th>24543</th>\n",
              "      <th>24544</th>\n",
              "      <th>24545</th>\n",
              "      <th>24546</th>\n",
              "      <th>24547</th>\n",
              "      <th>24548</th>\n",
              "      <th>24549</th>\n",
              "      <th>24550</th>\n",
              "      <th>24551</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>199.383575</td>\n",
              "      <td>199.383575</td>\n",
              "      <td>199.396896</td>\n",
              "      <td>199.410202</td>\n",
              "      <td>199.423523</td>\n",
              "      <td>199.436829</td>\n",
              "      <td>199.45015</td>\n",
              "      <td>199.46347</td>\n",
              "      <td>199.476791</td>\n",
              "      <td>199.490097</td>\n",
              "      <td>199.503418</td>\n",
              "      <td>199.516739</td>\n",
              "      <td>199.53006</td>\n",
              "      <td>199.543381</td>\n",
              "      <td>199.556702</td>\n",
              "      <td>199.570023</td>\n",
              "      <td>199.583344</td>\n",
              "      <td>199.596664</td>\n",
              "      <td>199.609985</td>\n",
              "      <td>199.623306</td>\n",
              "      <td>199.636627</td>\n",
              "      <td>199.649948</td>\n",
              "      <td>199.663269</td>\n",
              "      <td>199.67659</td>\n",
              "      <td>199.689926</td>\n",
              "      <td>199.703247</td>\n",
              "      <td>199.716568</td>\n",
              "      <td>199.729904</td>\n",
              "      <td>199.743225</td>\n",
              "      <td>199.756546</td>\n",
              "      <td>199.769882</td>\n",
              "      <td>199.783203</td>\n",
              "      <td>199.796524</td>\n",
              "      <td>199.80986</td>\n",
              "      <td>199.823181</td>\n",
              "      <td>199.836517</td>\n",
              "      <td>199.849854</td>\n",
              "      <td>199.863174</td>\n",
              "      <td>199.876511</td>\n",
              "      <td>199.889832</td>\n",
              "      <td>...</td>\n",
              "      <td>892.376526</td>\n",
              "      <td>892.429443</td>\n",
              "      <td>892.4823</td>\n",
              "      <td>892.535217</td>\n",
              "      <td>892.588074</td>\n",
              "      <td>892.640991</td>\n",
              "      <td>892.693909</td>\n",
              "      <td>892.746765</td>\n",
              "      <td>892.799683</td>\n",
              "      <td>892.8526</td>\n",
              "      <td>892.905518</td>\n",
              "      <td>892.958435</td>\n",
              "      <td>893.011353</td>\n",
              "      <td>893.06427</td>\n",
              "      <td>893.117187</td>\n",
              "      <td>893.170105</td>\n",
              "      <td>893.223022</td>\n",
              "      <td>893.27594</td>\n",
              "      <td>893.328857</td>\n",
              "      <td>893.381836</td>\n",
              "      <td>893.434753</td>\n",
              "      <td>893.487671</td>\n",
              "      <td>893.540649</td>\n",
              "      <td>893.593567</td>\n",
              "      <td>893.646545</td>\n",
              "      <td>893.699463</td>\n",
              "      <td>893.752441</td>\n",
              "      <td>893.80542</td>\n",
              "      <td>893.858337</td>\n",
              "      <td>893.911316</td>\n",
              "      <td>893.964294</td>\n",
              "      <td>894.017273</td>\n",
              "      <td>894.070251</td>\n",
              "      <td>894.123169</td>\n",
              "      <td>894.176147</td>\n",
              "      <td>894.229126</td>\n",
              "      <td>894.282104</td>\n",
              "      <td>894.335144</td>\n",
              "      <td>894.388123</td>\n",
              "      <td>894.441101</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24552 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0           1           2      ...       24549       24550       24551\n",
              "0  199.383575  199.383575  199.396896  ...  894.335144  894.388123  894.441101\n",
              "\n",
              "[1 rows x 24552 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW-5jgXVlycE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "4b4081be-ed4a-430d-aa59-32ac2ce853eb"
      },
      "source": [
        "import csv\n",
        "import glob\n",
        "\n",
        "\n",
        "path_nnt = \"/content/drive/My Drive/Covid/patients/covid/NNT\"\n",
        "path_nt = \"/content/drive/My Drive/Covid/patients/covid/NT\"\n",
        "path_p = \"/content/drive/My Drive/Covid/patients/covid/P\"\n",
        "\n",
        "lst = []\n",
        "\n",
        "filenames1 = glob.glob(path_nnt + \"/*.csv\")\n",
        "filenames2 = glob.glob(path_nt + \"/*.csv\")\n",
        "filenames3 = glob.glob(path_p + \"/*.csv\")\n",
        "\n",
        "\n",
        "dfs = []\n",
        "data_label = []\n",
        "for filename in filenames1:\n",
        "    frame = pd.read_csv(filename, header=None, usecols=[1]).T\n",
        "    dfs.append(frame)\n",
        "    data_label.append('Negative')\n",
        "\n",
        "# Concatenate all data into one DataFrame\n",
        "df_covid = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "for filename in filenames2:\n",
        "    frame = pd.read_csv(filename, header=None, usecols=[1]).T\n",
        "    dfs.append(frame)\n",
        "    data_label.append('Negative')\n",
        "\n",
        "# Concatenate all data into one DataFrame\n",
        "df_covid = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "for filename in filenames3:\n",
        "    frame = pd.read_csv(filename, header=None, usecols=[1]).T\n",
        "    dfs.append(frame)\n",
        "    data_label.append('Positive')\n",
        "\n",
        "# Concatenate all frames from files into one DataFrame\n",
        "df_covid = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# column names\n",
        "covid_features = np.round(cols.values.flatten(),decimal_round)\n",
        "df_covid.columns = covid_features\n",
        "\n",
        "\n",
        "# Adding the class labels to the dataset\n",
        "#df_covid['class_label'] = data_label\n",
        "\n",
        "## intersected features\n",
        "value = np.intersect1d(covid_features, rock_features)\n",
        "print(value)\n",
        "print(len(value))\n",
        "\n",
        "df_covid.shape\n",
        "#df_covid.value_counts(df_covid.class_label)\n",
        "df_covid.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[200.784 203.492 207.508 213.787 220.869 226.49  231.639 233.276 248.632\n",
            " 249.248 259.165 276.58  285.893 292.109 293.567 295.193 299.85  309.584\n",
            " 311.692 312.051 338.823 352.07  364.361 367.602 369.032 380.056 389.071\n",
            " 389.994 437.555 447.672 464.698 465.01  469.658 563.259 579.287 584.36\n",
            " 624.89  657.619 679.844 683.52  718.757 748.936 771.246 785.97  817.804]\n",
            "45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>199.3836</th>\n",
              "      <th>199.3836</th>\n",
              "      <th>199.3969</th>\n",
              "      <th>199.4102</th>\n",
              "      <th>199.4235</th>\n",
              "      <th>199.4368</th>\n",
              "      <th>199.4502</th>\n",
              "      <th>199.4635</th>\n",
              "      <th>199.4768</th>\n",
              "      <th>199.4901</th>\n",
              "      <th>199.5034</th>\n",
              "      <th>199.5167</th>\n",
              "      <th>199.5301</th>\n",
              "      <th>199.5434</th>\n",
              "      <th>199.5567</th>\n",
              "      <th>199.5700</th>\n",
              "      <th>199.5833</th>\n",
              "      <th>199.5967</th>\n",
              "      <th>199.6100</th>\n",
              "      <th>199.6233</th>\n",
              "      <th>199.6366</th>\n",
              "      <th>199.6499</th>\n",
              "      <th>199.6633</th>\n",
              "      <th>199.6766</th>\n",
              "      <th>199.6899</th>\n",
              "      <th>199.7032</th>\n",
              "      <th>199.7166</th>\n",
              "      <th>199.7299</th>\n",
              "      <th>199.7432</th>\n",
              "      <th>199.7565</th>\n",
              "      <th>199.7699</th>\n",
              "      <th>199.7832</th>\n",
              "      <th>199.7965</th>\n",
              "      <th>199.8099</th>\n",
              "      <th>199.8232</th>\n",
              "      <th>199.8365</th>\n",
              "      <th>199.8499</th>\n",
              "      <th>199.8632</th>\n",
              "      <th>199.8765</th>\n",
              "      <th>199.8898</th>\n",
              "      <th>...</th>\n",
              "      <th>892.3765</th>\n",
              "      <th>892.4294</th>\n",
              "      <th>892.4823</th>\n",
              "      <th>892.5352</th>\n",
              "      <th>892.5881</th>\n",
              "      <th>892.6410</th>\n",
              "      <th>892.6939</th>\n",
              "      <th>892.7468</th>\n",
              "      <th>892.7997</th>\n",
              "      <th>892.8526</th>\n",
              "      <th>892.9055</th>\n",
              "      <th>892.9584</th>\n",
              "      <th>893.0114</th>\n",
              "      <th>893.0643</th>\n",
              "      <th>893.1172</th>\n",
              "      <th>893.1701</th>\n",
              "      <th>893.2230</th>\n",
              "      <th>893.2759</th>\n",
              "      <th>893.3289</th>\n",
              "      <th>893.3818</th>\n",
              "      <th>893.4348</th>\n",
              "      <th>893.4877</th>\n",
              "      <th>893.5406</th>\n",
              "      <th>893.5936</th>\n",
              "      <th>893.6465</th>\n",
              "      <th>893.6995</th>\n",
              "      <th>893.7524</th>\n",
              "      <th>893.8054</th>\n",
              "      <th>893.8583</th>\n",
              "      <th>893.9113</th>\n",
              "      <th>893.9643</th>\n",
              "      <th>894.0173</th>\n",
              "      <th>894.0703</th>\n",
              "      <th>894.1232</th>\n",
              "      <th>894.1761</th>\n",
              "      <th>894.2291</th>\n",
              "      <th>894.2821</th>\n",
              "      <th>894.3351</th>\n",
              "      <th>894.3881</th>\n",
              "      <th>894.4411</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>483.662787</td>\n",
              "      <td>476.910714</td>\n",
              "      <td>453.212906</td>\n",
              "      <td>456.052689</td>\n",
              "      <td>441.438996</td>\n",
              "      <td>481.654421</td>\n",
              "      <td>476.272731</td>\n",
              "      <td>492.314996</td>\n",
              "      <td>473.168515</td>\n",
              "      <td>464.634412</td>\n",
              "      <td>442.993309</td>\n",
              "      <td>441.084325</td>\n",
              "      <td>435.791937</td>\n",
              "      <td>453.568387</td>\n",
              "      <td>433.774589</td>\n",
              "      <td>422.348348</td>\n",
              "      <td>438.141095</td>\n",
              "      <td>434.404507</td>\n",
              "      <td>429.103516</td>\n",
              "      <td>446.162612</td>\n",
              "      <td>432.881353</td>\n",
              "      <td>429.921458</td>\n",
              "      <td>423.072559</td>\n",
              "      <td>420.579250</td>\n",
              "      <td>432.666950</td>\n",
              "      <td>422.744376</td>\n",
              "      <td>415.316244</td>\n",
              "      <td>404.409967</td>\n",
              "      <td>418.994125</td>\n",
              "      <td>412.941858</td>\n",
              "      <td>403.977802</td>\n",
              "      <td>402.132270</td>\n",
              "      <td>420.450691</td>\n",
              "      <td>433.163994</td>\n",
              "      <td>441.011509</td>\n",
              "      <td>450.741366</td>\n",
              "      <td>419.190737</td>\n",
              "      <td>404.323416</td>\n",
              "      <td>416.846988</td>\n",
              "      <td>419.521334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>324.840109</td>\n",
              "      <td>326.419717</td>\n",
              "      <td>315.686290</td>\n",
              "      <td>306.358397</td>\n",
              "      <td>312.763534</td>\n",
              "      <td>332.916388</td>\n",
              "      <td>318.780190</td>\n",
              "      <td>324.899224</td>\n",
              "      <td>328.387800</td>\n",
              "      <td>326.730442</td>\n",
              "      <td>317.499862</td>\n",
              "      <td>333.099239</td>\n",
              "      <td>312.371791</td>\n",
              "      <td>317.317949</td>\n",
              "      <td>301.444064</td>\n",
              "      <td>298.475766</td>\n",
              "      <td>310.962694</td>\n",
              "      <td>322.267749</td>\n",
              "      <td>327.099062</td>\n",
              "      <td>323.205899</td>\n",
              "      <td>323.046249</td>\n",
              "      <td>318.737877</td>\n",
              "      <td>302.476458</td>\n",
              "      <td>304.822270</td>\n",
              "      <td>313.671079</td>\n",
              "      <td>296.645675</td>\n",
              "      <td>290.386600</td>\n",
              "      <td>293.556490</td>\n",
              "      <td>301.449117</td>\n",
              "      <td>287.925919</td>\n",
              "      <td>283.432818</td>\n",
              "      <td>279.857445</td>\n",
              "      <td>289.375066</td>\n",
              "      <td>296.591098</td>\n",
              "      <td>296.562544</td>\n",
              "      <td>308.054704</td>\n",
              "      <td>303.745725</td>\n",
              "      <td>304.238402</td>\n",
              "      <td>296.819652</td>\n",
              "      <td>275.647610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>334.928954</td>\n",
              "      <td>332.209867</td>\n",
              "      <td>313.814024</td>\n",
              "      <td>341.552440</td>\n",
              "      <td>360.842671</td>\n",
              "      <td>350.439148</td>\n",
              "      <td>340.103141</td>\n",
              "      <td>364.783511</td>\n",
              "      <td>420.201943</td>\n",
              "      <td>408.714480</td>\n",
              "      <td>347.626978</td>\n",
              "      <td>348.877267</td>\n",
              "      <td>374.727388</td>\n",
              "      <td>403.582732</td>\n",
              "      <td>383.090775</td>\n",
              "      <td>361.715773</td>\n",
              "      <td>339.066935</td>\n",
              "      <td>333.288014</td>\n",
              "      <td>368.388423</td>\n",
              "      <td>400.384241</td>\n",
              "      <td>380.161620</td>\n",
              "      <td>391.545550</td>\n",
              "      <td>396.405825</td>\n",
              "      <td>373.309477</td>\n",
              "      <td>362.813072</td>\n",
              "      <td>373.163958</td>\n",
              "      <td>390.972514</td>\n",
              "      <td>377.493210</td>\n",
              "      <td>356.238316</td>\n",
              "      <td>326.787920</td>\n",
              "      <td>323.315531</td>\n",
              "      <td>325.418033</td>\n",
              "      <td>352.269458</td>\n",
              "      <td>381.319702</td>\n",
              "      <td>371.483736</td>\n",
              "      <td>356.800814</td>\n",
              "      <td>340.691963</td>\n",
              "      <td>335.759475</td>\n",
              "      <td>340.420507</td>\n",
              "      <td>339.681964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>393.557389</td>\n",
              "      <td>394.281295</td>\n",
              "      <td>388.881964</td>\n",
              "      <td>378.240301</td>\n",
              "      <td>350.241074</td>\n",
              "      <td>367.864935</td>\n",
              "      <td>346.115008</td>\n",
              "      <td>354.866686</td>\n",
              "      <td>343.067883</td>\n",
              "      <td>346.727706</td>\n",
              "      <td>336.658425</td>\n",
              "      <td>357.418007</td>\n",
              "      <td>338.806958</td>\n",
              "      <td>347.211947</td>\n",
              "      <td>341.011411</td>\n",
              "      <td>337.216620</td>\n",
              "      <td>339.498419</td>\n",
              "      <td>343.424817</td>\n",
              "      <td>366.871391</td>\n",
              "      <td>365.808097</td>\n",
              "      <td>341.646956</td>\n",
              "      <td>352.425020</td>\n",
              "      <td>359.050551</td>\n",
              "      <td>351.549978</td>\n",
              "      <td>345.199564</td>\n",
              "      <td>334.600315</td>\n",
              "      <td>326.789848</td>\n",
              "      <td>317.845705</td>\n",
              "      <td>328.875343</td>\n",
              "      <td>326.041619</td>\n",
              "      <td>324.053942</td>\n",
              "      <td>314.773719</td>\n",
              "      <td>328.241466</td>\n",
              "      <td>334.387000</td>\n",
              "      <td>329.445425</td>\n",
              "      <td>346.836456</td>\n",
              "      <td>361.999827</td>\n",
              "      <td>366.275368</td>\n",
              "      <td>346.716814</td>\n",
              "      <td>308.381076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>636.501821</td>\n",
              "      <td>615.197078</td>\n",
              "      <td>606.966437</td>\n",
              "      <td>634.345813</td>\n",
              "      <td>617.315981</td>\n",
              "      <td>633.726760</td>\n",
              "      <td>571.190957</td>\n",
              "      <td>589.950598</td>\n",
              "      <td>586.916204</td>\n",
              "      <td>559.490768</td>\n",
              "      <td>525.762634</td>\n",
              "      <td>543.156631</td>\n",
              "      <td>512.789800</td>\n",
              "      <td>541.377921</td>\n",
              "      <td>539.717594</td>\n",
              "      <td>529.479025</td>\n",
              "      <td>519.583588</td>\n",
              "      <td>494.103353</td>\n",
              "      <td>511.107791</td>\n",
              "      <td>524.356027</td>\n",
              "      <td>492.604975</td>\n",
              "      <td>487.155847</td>\n",
              "      <td>484.668104</td>\n",
              "      <td>474.960600</td>\n",
              "      <td>472.520070</td>\n",
              "      <td>474.229122</td>\n",
              "      <td>476.590604</td>\n",
              "      <td>460.869790</td>\n",
              "      <td>463.780364</td>\n",
              "      <td>448.108058</td>\n",
              "      <td>453.681536</td>\n",
              "      <td>447.229871</td>\n",
              "      <td>444.792393</td>\n",
              "      <td>444.333213</td>\n",
              "      <td>453.283266</td>\n",
              "      <td>476.332199</td>\n",
              "      <td>479.176006</td>\n",
              "      <td>473.638032</td>\n",
              "      <td>440.429770</td>\n",
              "      <td>405.469471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24552 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   199.3836  199.3836  199.3969  ...    894.3351    894.3881    894.4411\n",
              "0       0.0       0.0       0.0  ...  404.323416  416.846988  419.521334\n",
              "1       0.0       0.0       0.0  ...  304.238402  296.819652  275.647610\n",
              "2       0.0       0.0       0.0  ...  335.759475  340.420507  339.681964\n",
              "3       0.0       0.0       0.0  ...  366.275368  346.716814  308.381076\n",
              "4       0.0       0.0       0.0  ...  473.638032  440.429770  405.469471\n",
              "\n",
              "[5 rows x 24552 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozTyipK3MnIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d744f971-6a75-487e-83d1-9583b6a59d48"
      },
      "source": [
        "from pandas import read_csv\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Conv1D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "ylab = data_label\n",
        "le = LabelEncoder()\n",
        "le.fit(ylab)\n",
        "y = le.transform(ylab)\n",
        "\n",
        "one_hot_label = to_categorical(y)\n",
        "\n",
        "X = df_covid\n",
        "\n",
        "x = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,one_hot_label,test_size=0.2)\n",
        "print(x.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(16, 2, activation=\"relu\", input_shape=(24552, 1)))\n",
        "model.add(Conv1D(8, 2, activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax')) \n",
        "model.summary()\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_acc', patience=0, verbose=1 )\n",
        "# patience = n n=number of epochs without any improvements\n",
        "\n",
        "history=model.fit(X_train, y_train,\n",
        "          epochs=200,\n",
        "          validation_split=0.15,\n",
        "          callbacks=[stop_early],\n",
        "          shuffle=True) \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(72, 24552, 1)\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_32 (Conv1D)           (None, 24551, 16)         48        \n",
            "_________________________________________________________________\n",
            "conv1d_33 (Conv1D)           (None, 24550, 8)          264       \n",
            "_________________________________________________________________\n",
            "flatten_37 (Flatten)         (None, 196400)            0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 2)                 392802    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 393,114\n",
            "Trainable params: 393,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 31889.4863 - accuracy: 0.4167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 31889.4863 - accuracy: 0.4167 - val_loss: 46819.0156 - val_accuracy: 0.1111\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 35043.8906 - accuracy: 0.3542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 35043.8906 - accuracy: 0.3542 - val_loss: 1612.1219 - val_accuracy: 0.8889\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 12932.1904 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 12932.1904 - accuracy: 0.6458 - val_loss: 1315.3351 - val_accuracy: 0.8889\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 8981.4922 - accuracy: 0.6458 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 8981.4922 - accuracy: 0.6458 - val_loss: 10384.3936 - val_accuracy: 0.1111\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 10094.2939 - accuracy: 0.3542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 10094.2939 - accuracy: 0.3542 - val_loss: 755.6248 - val_accuracy: 0.5556\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1862.9178 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 1862.9178 - accuracy: 0.6667 - val_loss: 762.8820 - val_accuracy: 0.8889\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 7341.5532 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 7341.5532 - accuracy: 0.6458 - val_loss: 395.0178 - val_accuracy: 0.7778\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3048.5051 - accuracy: 0.5417WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 3048.5051 - accuracy: 0.5417 - val_loss: 9160.4307 - val_accuracy: 0.2222\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 5799.0122 - accuracy: 0.4375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 5799.0122 - accuracy: 0.4375 - val_loss: 279.1154 - val_accuracy: 0.6667\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 6626.9888 - accuracy: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 6626.9888 - accuracy: 0.6250 - val_loss: 622.4350 - val_accuracy: 0.8889\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 5965.3540 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 5965.3540 - accuracy: 0.6458 - val_loss: 2449.8499 - val_accuracy: 0.4444\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2298.9031 - accuracy: 0.5833WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 2298.9031 - accuracy: 0.5833 - val_loss: 7476.7568 - val_accuracy: 0.3333\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 4208.7515 - accuracy: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 4208.7515 - accuracy: 0.6250 - val_loss: 799.8315 - val_accuracy: 0.7778\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2094.2295 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 2094.2295 - accuracy: 0.6667 - val_loss: 466.3221 - val_accuracy: 0.6667\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2231.5789 - accuracy: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 2231.5789 - accuracy: 0.6250 - val_loss: 3426.5022 - val_accuracy: 0.3333\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3320.6838 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 3320.6838 - accuracy: 0.6458 - val_loss: 6218.1333 - val_accuracy: 0.3333\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2968.7139 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 2968.7139 - accuracy: 0.6458 - val_loss: 525.7640 - val_accuracy: 0.6667\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2268.5315 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 2268.5315 - accuracy: 0.6458 - val_loss: 479.2547 - val_accuracy: 0.6667\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1884.8282 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 1884.8282 - accuracy: 0.6458 - val_loss: 2451.3315 - val_accuracy: 0.3333\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1579.7319 - accuracy: 0.6875WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 1579.7319 - accuracy: 0.6875 - val_loss: 4252.8330 - val_accuracy: 0.3333\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1806.5708 - accuracy: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 1806.5708 - accuracy: 0.6250 - val_loss: 476.4492 - val_accuracy: 0.6667\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2227.5410 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 2227.5410 - accuracy: 0.6667 - val_loss: 476.3428 - val_accuracy: 0.6667\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1365.6061 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 1365.6061 - accuracy: 0.7292 - val_loss: 3375.7236 - val_accuracy: 0.3333\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1825.6849 - accuracy: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 1825.6849 - accuracy: 0.6250 - val_loss: 1378.2826 - val_accuracy: 0.5556\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 671.5869 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 671.5869 - accuracy: 0.7708 - val_loss: 467.4115 - val_accuracy: 0.6667\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1158.4987 - accuracy: 0.7083WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 1158.4987 - accuracy: 0.7083 - val_loss: 874.6177 - val_accuracy: 0.4444\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 586.6736 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 586.6736 - accuracy: 0.7708 - val_loss: 1601.0499 - val_accuracy: 0.4444\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 695.8350 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 695.8350 - accuracy: 0.7500 - val_loss: 467.2707 - val_accuracy: 0.6667\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 608.6414 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 608.6414 - accuracy: 0.7708 - val_loss: 731.4485 - val_accuracy: 0.4444\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 586.1297 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 586.1297 - accuracy: 0.7500 - val_loss: 907.1120 - val_accuracy: 0.4444\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 461.9791 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 461.9791 - accuracy: 0.7292 - val_loss: 429.4953 - val_accuracy: 0.6667\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 346.1715 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 346.1715 - accuracy: 0.8125 - val_loss: 1108.8781 - val_accuracy: 0.3333\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 454.5856 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 454.5856 - accuracy: 0.7292 - val_loss: 514.1022 - val_accuracy: 0.4444\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 152.3559 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 152.3559 - accuracy: 0.8750 - val_loss: 539.2644 - val_accuracy: 0.8889\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 596.0520 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 596.0520 - accuracy: 0.7500 - val_loss: 446.4772 - val_accuracy: 0.6667\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 230.2836 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 230.2836 - accuracy: 0.8750 - val_loss: 859.9769 - val_accuracy: 0.3333\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 334.1360 - accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 334.1360 - accuracy: 0.7917 - val_loss: 437.8479 - val_accuracy: 0.8889\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 395.9470 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 395.9470 - accuracy: 0.8333 - val_loss: 539.1851 - val_accuracy: 0.8889\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 282.0412 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 282.0412 - accuracy: 0.8333 - val_loss: 2186.5063 - val_accuracy: 0.3333\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1325.9092 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 1325.9092 - accuracy: 0.6667 - val_loss: 405.5762 - val_accuracy: 0.7778\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 803.5881 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 803.5881 - accuracy: 0.7708 - val_loss: 657.1745 - val_accuracy: 0.8889\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 653.6231 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 653.6231 - accuracy: 0.7500 - val_loss: 2405.0981 - val_accuracy: 0.3333\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1439.1637 - accuracy: 0.6042WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 1439.1637 - accuracy: 0.6042 - val_loss: 415.1676 - val_accuracy: 0.6667\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 655.8318 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 655.8318 - accuracy: 0.7500 - val_loss: 577.4846 - val_accuracy: 0.8889\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 780.6094 - accuracy: 0.7500 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 780.6094 - accuracy: 0.7500 - val_loss: 1615.3540 - val_accuracy: 0.3333\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 926.9709 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 926.9709 - accuracy: 0.7500 - val_loss: 462.3790 - val_accuracy: 0.6667\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 407.6473 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 407.6473 - accuracy: 0.8333 - val_loss: 545.7404 - val_accuracy: 0.8889\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 696.6733 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 696.6733 - accuracy: 0.7292 - val_loss: 1512.3777 - val_accuracy: 0.3333\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 712.2563 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 712.2563 - accuracy: 0.7292 - val_loss: 1295.8960 - val_accuracy: 0.3333\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 408.8398 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 408.8398 - accuracy: 0.8125 - val_loss: 520.6121 - val_accuracy: 0.8889\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 920.7731 - accuracy: 0.6875 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 920.7731 - accuracy: 0.6875 - val_loss: 453.9329 - val_accuracy: 0.6667\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 360.6932 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 360.6932 - accuracy: 0.8542 - val_loss: 1100.8265 - val_accuracy: 0.4444\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 423.5624 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 423.5624 - accuracy: 0.8333 - val_loss: 459.9959 - val_accuracy: 0.6667\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 222.0728 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 222.0728 - accuracy: 0.8542 - val_loss: 413.4926 - val_accuracy: 0.6667\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 316.0006 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 316.0006 - accuracy: 0.8750 - val_loss: 400.9719 - val_accuracy: 0.6667\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 299.5616 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 299.5616 - accuracy: 0.8958 - val_loss: 569.4575 - val_accuracy: 0.4444\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 245.5038 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 245.5038 - accuracy: 0.8750 - val_loss: 618.8588 - val_accuracy: 0.4444\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 213.7220 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 213.7220 - accuracy: 0.8333 - val_loss: 417.3315 - val_accuracy: 0.8889\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 302.1512 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 302.1512 - accuracy: 0.8750 - val_loss: 417.0251 - val_accuracy: 0.8889\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 192.3863 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 192.3863 - accuracy: 0.9167 - val_loss: 540.3232 - val_accuracy: 0.4444\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 213.6995 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 213.6995 - accuracy: 0.8542 - val_loss: 373.1371 - val_accuracy: 0.6667\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 154.4967 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 154.4967 - accuracy: 0.8958 - val_loss: 461.1825 - val_accuracy: 0.8889\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 179.2960 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 179.2960 - accuracy: 0.8750 - val_loss: 405.9028 - val_accuracy: 0.6667\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 264.2142 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 264.2142 - accuracy: 0.8333 - val_loss: 487.4182 - val_accuracy: 0.8889\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 326.3438 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 326.3438 - accuracy: 0.8750 - val_loss: 551.0495 - val_accuracy: 0.8889\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 157.0760 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 157.0760 - accuracy: 0.9167 - val_loss: 1445.7311 - val_accuracy: 0.3333\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 572.3482 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 572.3482 - accuracy: 0.7292 - val_loss: 521.6222 - val_accuracy: 0.8889\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 218.6327 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 218.6327 - accuracy: 0.8542 - val_loss: 673.9290 - val_accuracy: 0.8889\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 514.8667 - accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 514.8667 - accuracy: 0.7917 - val_loss: 830.7896 - val_accuracy: 0.4444\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 246.9078 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 246.9078 - accuracy: 0.8333 - val_loss: 447.9733 - val_accuracy: 0.8889\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 149.4264 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 149.4264 - accuracy: 0.8958 - val_loss: 489.4494 - val_accuracy: 0.8889\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 206.6176 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 206.6176 - accuracy: 0.8958 - val_loss: 616.9856 - val_accuracy: 0.6667\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 155.7535 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 107ms/step - loss: 155.7535 - accuracy: 0.8750 - val_loss: 510.9553 - val_accuracy: 0.6667\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 146.8060 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 146.8060 - accuracy: 0.8958 - val_loss: 456.5065 - val_accuracy: 0.8889\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 136.8218 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 136.8218 - accuracy: 0.8958 - val_loss: 513.0130 - val_accuracy: 0.8889\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 142.0604 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 142.0604 - accuracy: 0.9167 - val_loss: 479.0775 - val_accuracy: 0.8889\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 152.3384 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 152.3384 - accuracy: 0.9167 - val_loss: 603.1356 - val_accuracy: 0.6667\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 133.1903 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 133.1903 - accuracy: 0.8958 - val_loss: 582.9944 - val_accuracy: 0.6667\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 127.1817 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 127.1817 - accuracy: 0.8958 - val_loss: 448.7496 - val_accuracy: 0.8889\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 115.8876 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 115.8876 - accuracy: 0.8958 - val_loss: 497.7748 - val_accuracy: 0.8889\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 129.7246 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 129.7246 - accuracy: 0.8958 - val_loss: 504.2312 - val_accuracy: 0.6667\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 96.8220 - accuracy: 0.8958 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 96.8220 - accuracy: 0.8958 - val_loss: 1091.6278 - val_accuracy: 0.4444\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 365.1300 - accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 365.1300 - accuracy: 0.7917 - val_loss: 562.9208 - val_accuracy: 0.8889\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 253.5420 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 253.5420 - accuracy: 0.8542 - val_loss: 423.7707 - val_accuracy: 0.8889\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 75.6886 - accuracy: 0.9167 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 75.6886 - accuracy: 0.9167 - val_loss: 1367.0886 - val_accuracy: 0.4444\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 450.4380 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 450.4380 - accuracy: 0.7708 - val_loss: 532.1086 - val_accuracy: 0.8889\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 313.5181 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 313.5181 - accuracy: 0.8542 - val_loss: 445.0731 - val_accuracy: 0.8889\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 217.9863 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 217.9863 - accuracy: 0.8750 - val_loss: 800.1917 - val_accuracy: 0.4444\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 236.1591 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 236.1591 - accuracy: 0.8750 - val_loss: 414.1818 - val_accuracy: 0.8889\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 119.8214 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 119.8214 - accuracy: 0.8750 - val_loss: 350.6041 - val_accuracy: 0.8889\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 96.7711 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 96.7711 - accuracy: 0.9167 - val_loss: 477.6190 - val_accuracy: 0.6667\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 99.1493 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 99.1493 - accuracy: 0.8750 - val_loss: 409.0502 - val_accuracy: 0.8889\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 137.6676 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 137.6676 - accuracy: 0.8958 - val_loss: 379.8976 - val_accuracy: 0.8889\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 93.8482 - accuracy: 0.8958 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 93.8482 - accuracy: 0.8958 - val_loss: 617.3448 - val_accuracy: 0.6667\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 87.4074 - accuracy: 0.8542 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 87.4074 - accuracy: 0.8542 - val_loss: 510.4996 - val_accuracy: 0.8889\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 219.0788 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 219.0788 - accuracy: 0.8542 - val_loss: 409.6556 - val_accuracy: 0.7778\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 328.3551 - accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 328.3551 - accuracy: 0.7917 - val_loss: 539.5247 - val_accuracy: 0.8889\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 415.5508 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 415.5508 - accuracy: 0.8125 - val_loss: 609.9371 - val_accuracy: 0.8889\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 601.3564 - accuracy: 0.7083WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 601.3564 - accuracy: 0.7083 - val_loss: 1864.0386 - val_accuracy: 0.3333\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 983.4417 - accuracy: 0.6875 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 983.4417 - accuracy: 0.6875 - val_loss: 633.9435 - val_accuracy: 0.8889\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 746.7305 - accuracy: 0.6875WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 746.7305 - accuracy: 0.6875 - val_loss: 514.2913 - val_accuracy: 0.6667\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 567.5187 - accuracy: 0.7083WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 567.5187 - accuracy: 0.7083 - val_loss: 411.8484 - val_accuracy: 0.8889\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 436.4072 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 436.4072 - accuracy: 0.8333 - val_loss: 586.4316 - val_accuracy: 0.8889\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 522.5096 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 522.5096 - accuracy: 0.7292 - val_loss: 2329.1958 - val_accuracy: 0.3333\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1159.5526 - accuracy: 0.6042WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 1159.5526 - accuracy: 0.6042 - val_loss: 502.8155 - val_accuracy: 0.8889\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 844.1437 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 844.1437 - accuracy: 0.7500 - val_loss: 606.6650 - val_accuracy: 0.8889\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 807.6475 - accuracy: 0.7500 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 807.6475 - accuracy: 0.7500 - val_loss: 3409.8459 - val_accuracy: 0.3333\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2339.9189 - accuracy: 0.4792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2339.9189 - accuracy: 0.4792 - val_loss: 552.9385 - val_accuracy: 0.8889\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1515.9479 - accuracy: 0.6875WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 1515.9479 - accuracy: 0.6875 - val_loss: 895.9576 - val_accuracy: 0.8889\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2676.2673 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2676.2673 - accuracy: 0.6667 - val_loss: 574.0421 - val_accuracy: 0.4444\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1062.2863 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 1062.2863 - accuracy: 0.7708 - val_loss: 4011.3628 - val_accuracy: 0.3333\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1738.0907 - accuracy: 0.6042WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 1738.0907 - accuracy: 0.6042 - val_loss: 564.3998 - val_accuracy: 0.8889\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1880.5879 - accuracy: 0.6458WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 1880.5879 - accuracy: 0.6458 - val_loss: 645.6697 - val_accuracy: 0.8889\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1353.0402 - accuracy: 0.7292WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 1353.0402 - accuracy: 0.7292 - val_loss: 1774.7637 - val_accuracy: 0.3333\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 884.7734 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 884.7734 - accuracy: 0.6667 - val_loss: 1585.7272 - val_accuracy: 0.3333\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 440.3742 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 440.3742 - accuracy: 0.8125 - val_loss: 409.4532 - val_accuracy: 0.8889\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 635.9587 - accuracy: 0.7500WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 635.9587 - accuracy: 0.7500 - val_loss: 424.7883 - val_accuracy: 0.8889\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 464.4959 - accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 464.4959 - accuracy: 0.7708 - val_loss: 1108.8271 - val_accuracy: 0.4444\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 383.3656 - accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 383.3656 - accuracy: 0.7917 - val_loss: 870.3306 - val_accuracy: 0.4444\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 190.6444 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 190.6444 - accuracy: 0.8542 - val_loss: 380.8762 - val_accuracy: 0.8889\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 284.7512 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 284.7512 - accuracy: 0.8542 - val_loss: 473.7786 - val_accuracy: 0.8889\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 404.7220 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 404.7220 - accuracy: 0.8542 - val_loss: 346.7989 - val_accuracy: 0.8889\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 127.6354 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 127.6354 - accuracy: 0.9375 - val_loss: 619.6052 - val_accuracy: 0.4444\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 162.7872 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 162.7872 - accuracy: 0.8958 - val_loss: 1028.2941 - val_accuracy: 0.4444\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 267.4354 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 267.4354 - accuracy: 0.8542 - val_loss: 320.5818 - val_accuracy: 0.7778\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 118.3969 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 118.3969 - accuracy: 0.9167 - val_loss: 480.7598 - val_accuracy: 0.8889\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 240.4683 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 240.4683 - accuracy: 0.9167 - val_loss: 470.5737 - val_accuracy: 0.8889\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 152.9046 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 152.9046 - accuracy: 0.9167 - val_loss: 393.0513 - val_accuracy: 0.6667\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 78.3003 - accuracy: 0.8958 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 78.3003 - accuracy: 0.8958 - val_loss: 1038.3506 - val_accuracy: 0.4444\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 224.9670 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 224.9670 - accuracy: 0.8125 - val_loss: 410.2263 - val_accuracy: 0.8889\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 88.6429 - accuracy: 0.9167 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 88.6429 - accuracy: 0.9167 - val_loss: 574.3097 - val_accuracy: 0.8889\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 274.7810 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 274.7810 - accuracy: 0.8750 - val_loss: 551.4247 - val_accuracy: 0.8889\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 197.2465 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 197.2465 - accuracy: 0.9167 - val_loss: 508.8107 - val_accuracy: 0.6667\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 107.5006 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 107.5006 - accuracy: 0.8958 - val_loss: 861.2697 - val_accuracy: 0.5556\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 124.7615 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 124.7615 - accuracy: 0.8750 - val_loss: 445.3612 - val_accuracy: 0.8889\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 98.9066 - accuracy: 0.8750 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 98.9066 - accuracy: 0.8750 - val_loss: 536.6298 - val_accuracy: 0.8889\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 150.3206 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 150.3206 - accuracy: 0.9375 - val_loss: 466.4626 - val_accuracy: 0.8889\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 48.3111 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 48.3111 - accuracy: 0.9583 - val_loss: 713.4700 - val_accuracy: 0.6667\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 90.0612 - accuracy: 0.8958 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 90.0612 - accuracy: 0.8958 - val_loss: 597.9301 - val_accuracy: 0.6667\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 58.2807 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 58.2807 - accuracy: 0.8958 - val_loss: 464.1642 - val_accuracy: 0.8889\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 63.4533 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 63.4533 - accuracy: 0.9375 - val_loss: 501.0614 - val_accuracy: 0.8889\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 81.7605 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 81.7605 - accuracy: 0.9583 - val_loss: 464.7738 - val_accuracy: 0.8889\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 70.0655 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 70.0655 - accuracy: 0.8958 - val_loss: 469.4359 - val_accuracy: 0.7778\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 49.4566 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 49.4566 - accuracy: 0.9375 - val_loss: 459.4121 - val_accuracy: 0.7778\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 30.7769 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 30.7769 - accuracy: 0.9375 - val_loss: 493.9072 - val_accuracy: 0.8889\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 48.0722 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 48.0722 - accuracy: 0.9583 - val_loss: 509.9536 - val_accuracy: 0.8889\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 41.0311 - accuracy: 0.9583   WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 41.0311 - accuracy: 0.9583 - val_loss: 618.7317 - val_accuracy: 0.6667\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 59.7397 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 59.7397 - accuracy: 0.9375 - val_loss: 684.5095 - val_accuracy: 0.6667\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 4.1741 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 4.1741 - accuracy: 0.9792 - val_loss: 558.9064 - val_accuracy: 0.8889\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 49.7398 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 49.7398 - accuracy: 0.9375 - val_loss: 551.1160 - val_accuracy: 0.8889\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 27.9970 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 27.9970 - accuracy: 0.9375 - val_loss: 701.0089 - val_accuracy: 0.6667\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 27.8298 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 27.8298 - accuracy: 0.9375 - val_loss: 966.8622 - val_accuracy: 0.6667\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 107.2687 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 107.2687 - accuracy: 0.8958 - val_loss: 573.4155 - val_accuracy: 0.8889\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 87.0936 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 87.0936 - accuracy: 0.9375 - val_loss: 704.2104 - val_accuracy: 0.8889\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 275.9064 - accuracy: 0.8125WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 275.9064 - accuracy: 0.8125 - val_loss: 507.3506 - val_accuracy: 0.8889\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 80.2739 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 80.2739 - accuracy: 0.8958 - val_loss: 1063.8158 - val_accuracy: 0.6667\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 198.7821 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 198.7821 - accuracy: 0.8542 - val_loss: 441.9589 - val_accuracy: 0.8889\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 88.0840 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 88.0840 - accuracy: 0.9375 - val_loss: 572.2216 - val_accuracy: 0.8889\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 195.4585 - accuracy: 0.8958WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 195.4585 - accuracy: 0.8958 - val_loss: 498.9086 - val_accuracy: 0.8889\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 82.2490 - accuracy: 0.9375 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 82.2490 - accuracy: 0.9375 - val_loss: 648.2726 - val_accuracy: 0.6667\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 158.4875 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 158.4875 - accuracy: 0.8333 - val_loss: 430.5184 - val_accuracy: 0.7778\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 13.7851 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 13.7851 - accuracy: 0.9583 - val_loss: 563.1915 - val_accuracy: 0.8889\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 217.0560 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 217.0560 - accuracy: 0.8542 - val_loss: 523.9595 - val_accuracy: 0.8889\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 142.0632 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 142.0632 - accuracy: 0.9167 - val_loss: 417.3141 - val_accuracy: 0.7778\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.9188 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 21.9188 - accuracy: 0.9375 - val_loss: 816.4463 - val_accuracy: 0.6667\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 130.3661 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 130.3661 - accuracy: 0.8542 - val_loss: 415.3284 - val_accuracy: 0.7778\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 77.3447 - accuracy: 0.9583   WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 77.3447 - accuracy: 0.9583 - val_loss: 573.6439 - val_accuracy: 0.8889\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 176.0334 - accuracy: 0.8750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 176.0334 - accuracy: 0.8750 - val_loss: 503.5346 - val_accuracy: 0.8889\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 60.3562 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 60.3562 - accuracy: 0.9375 - val_loss: 696.7100 - val_accuracy: 0.6667\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 80.5413 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 80.5413 - accuracy: 0.8333 - val_loss: 443.1876 - val_accuracy: 0.7778\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.3945 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 19.3945 - accuracy: 0.9375 - val_loss: 591.1240 - val_accuracy: 0.8889\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 195.6033 - accuracy: 0.8542WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 195.6033 - accuracy: 0.8542 - val_loss: 537.1776 - val_accuracy: 0.8889\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 60.6879 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 60.6879 - accuracy: 0.9167 - val_loss: 868.3102 - val_accuracy: 0.6667\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 106.9975 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 106.9975 - accuracy: 0.8333 - val_loss: 491.9111 - val_accuracy: 0.8889\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 11.5111 - accuracy: 0.9792   WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 11.5111 - accuracy: 0.9792 - val_loss: 733.2635 - val_accuracy: 0.8889\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 678.1609 - accuracy: 0.7708 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 678.1609 - accuracy: 0.7708 - val_loss: 423.3828 - val_accuracy: 0.8889\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 12.9278 - accuracy: 0.9583   WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 12.9278 - accuracy: 0.9583 - val_loss: 659.4711 - val_accuracy: 0.6667\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 25.5295 - accuracy: 0.9375WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 25.5295 - accuracy: 0.9375 - val_loss: 505.4763 - val_accuracy: 0.6667\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 15.1447 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 15.1447 - accuracy: 0.9792 - val_loss: 358.1751 - val_accuracy: 0.8889\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 11.1455 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 11.1455 - accuracy: 0.9583 - val_loss: 365.7800 - val_accuracy: 0.8889\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 25.6790 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 25.6790 - accuracy: 0.9583 - val_loss: 358.5985 - val_accuracy: 0.8889\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 25.4966 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 25.4966 - accuracy: 0.9583 - val_loss: 338.1349 - val_accuracy: 0.8889\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 16.5854 - accuracy: 0.9583WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 16.5854 - accuracy: 0.9583 - val_loss: 339.7526 - val_accuracy: 0.7778\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 9.5703 - accuracy: 0.9792 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 9.5703 - accuracy: 0.9792 - val_loss: 375.7205 - val_accuracy: 0.7778\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 10.8981 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 10.8981 - accuracy: 0.9792 - val_loss: 387.9219 - val_accuracy: 0.7778\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 8.7386 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 8.7386 - accuracy: 0.9792 - val_loss: 377.2612 - val_accuracy: 0.7778\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 5.1272 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 5.1272 - accuracy: 0.9792 - val_loss: 357.8178 - val_accuracy: 0.8889\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 4.3358 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 4.3358 - accuracy: 0.9792 - val_loss: 361.5909 - val_accuracy: 0.8889\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.0058 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 1.0058 - accuracy: 0.9792 - val_loss: 422.0130 - val_accuracy: 0.7778\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2.8149 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8149 - accuracy: 0.9792 - val_loss: 459.1409 - val_accuracy: 0.7778\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9586 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 0.9586 - accuracy: 0.9792 - val_loss: 450.3322 - val_accuracy: 0.7778\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.7757 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 1.7757 - accuracy: 0.9792 - val_loss: 445.0412 - val_accuracy: 0.7778\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.4200 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 131ms/step - loss: 1.4200 - accuracy: 0.9792 - val_loss: 478.8418 - val_accuracy: 0.7778\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 507.2448 - val_accuracy: 0.7778\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3.4133 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 3.4133 - accuracy: 0.9792 - val_loss: 495.4221 - val_accuracy: 0.7778\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 453.9709 - val_accuracy: 0.7778\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.9792WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 0.4745 - accuracy: 0.9792 - val_loss: 440.1119 - val_accuracy: 0.7778\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.2649 - accuracy: 0.9792    WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 1.2649 - accuracy: 0.9792 - val_loss: 449.8300 - val_accuracy: 0.7778\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 115ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 477.2095 - val_accuracy: 0.7778\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 497.7079 - val_accuracy: 0.7778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-pzQEGUFeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8680708-91f6-4a8f-e06c-9c96df4d58bf"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss: %.2f' %scores[0])\n",
        "print('Test accuracy: %0.2f' %scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 457.8800 - accuracy: 0.6667\n",
            "Test loss: 457.88\n",
            "Test accuracy: 0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZUyFIt2e0Oq"
      },
      "source": [
        "def plot_train_and_val(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'Validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "# call teh function we just created to plot validation and test accuracy/loss \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iwDoZmdLi2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "69515d77-09cf-43b0-80a0-c7946e625f59"
      },
      "source": [
        "plot_train_and_val(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3TZG9rELrV3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awLrC002ZwNb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byLIy8kje3sY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2wIHIX7BGXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}